{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lasso with Cross-validation\n\nThis example shows how to perform hyperparameter optimization\nfor a Lasso using a full cross-validation score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>\n#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>\n#          Mathurin Massias\n\n# License: BSD (3-clause)\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nfrom libsvmdata import fetch_libsvm\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import KFold\n\nfrom sparse_ho import ImplicitForward, grad_search\nfrom sparse_ho.models import Lasso\nfrom sparse_ho.criterion import HeldOutMSE, CrossVal\nfrom sparse_ho.optimizers import GradientDescent\nfrom sparse_ho.utils import Monitor\nfrom sparse_ho.utils_plot import discrete_cmap\n\nprint(__doc__)\n\n# dataset = 'rcv1'\ndataset = 'simu'\n\nif dataset == 'rcv1':\n    X, y = fetch_libsvm('rcv1_train')\nelse:\n    X, y = make_regression(\n        n_samples=500, n_features=1000, noise=40,\n        random_state=42)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nprint(\"Starting path computation...\")\nn_samples = len(y)\nalpha_max = np.max(np.abs(X.T.dot(y))) / n_samples\n\nn_alphas = 10\nalphas = np.geomspace(alpha_max, alpha_max / 1_000, n_alphas)\n\ntol = 1e-8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validation with scikit-learn\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('scikit started')\n\nt0 = time.time()\nreg = LassoCV(\n    cv=kf, verbose=True, tol=tol, fit_intercept=False,\n    alphas=alphas, max_iter=1e5).fit(X, y)\nreg.score(X, y)\nt_sk = time.time() - t0\n\nprint('scikit finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now do the hyperparameter optimization with implicit differentiation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = sklearn.linear_model.Lasso(\n    fit_intercept=False, max_iter=1000, warm_start=True, tol=tol)\n\nprint('sparse-ho started')\n\nt0 = time.time()\nmodel = Lasso()\ncriterion = HeldOutMSE(None, None)\nalpha0 = 0.9 * alpha_max\nmonitor_grad = Monitor()\ncross_val_criterion = CrossVal(criterion, cv=kf)\nalgo = ImplicitForward()\noptimizer = GradientDescent(n_outer=10, tol=tol)\ngrad_search(\n    algo, cross_val_criterion, model, optimizer, X, y, alpha0,\n    monitor_grad)\n\nt_grad_search = time.time() - t0\n\nprint('sparse-ho finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "objs = reg.mse_path_.mean(axis=1)\n\np_alphas_grad = np.array(monitor_grad.alphas) / alpha_max\nobjs_grad = np.array(monitor_grad.objs)\n\n\nprint(f\"Time for grid search: {t_sk:.2f} s\")\nprint(f\"Time for grad search (sparse-ho): {t_grad_search:.2f} s\")\n\nprint(f'Minimum outer criterion value with grid search: {objs.min():.5f}')\nprint(f'Minimum outer criterion value with grad search: {objs_grad.min():.5f}')\n\ncurrent_palette = sns.color_palette(\"colorblind\")\ncmap = discrete_cmap(len(objs_grad), 'Greens')\n\nfig, ax = plt.subplots(figsize=(5, 3))\nax.plot(alphas / alphas[0], objs, color=current_palette[0])\nax.plot(\n    alphas / alphas[0], objs,\n    'bo', label='0-th order method (grid search)',\n    color=current_palette[1])\nax.scatter(\n    p_alphas_grad, objs_grad,\n    label='1-st order method',  marker='X',\n    color=cmap(np.linspace(0, 1, len(objs_grad))), s=40, zorder=40)\nplt.xlabel(r\"$\\lambda / \\lambda_{\\max}$\")\nplt.ylabel(\"(Normalized) Cross-validation loss\")\nax.set_xscale(\"log\")\nplt.tick_params(width=5)\nplt.legend()\nplt.tight_layout()\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}