{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Weighted Lasso with held-out test set\n\nThis example shows how to perform hyperparameter optimization\nfor a weighted Lasso using a held-out validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>\n#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>\n#\n# License: BSD (3-clause)\n\nimport time\n\nimport numpy as np\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nfrom celer import Lasso\n\nfrom sparse_ho.models import wLasso\nfrom sparse_ho.criterion import CV\nfrom sparse_ho.implicit_forward import ImplicitForward\nfrom sparse_ho.utils import Monitor\nfrom sparse_ho.ho import grad_search\nfrom sparse_ho.datasets import get_data\n\nprint(__doc__)\n\ndataset = 'rcv1_train'\n# dataset = 'simu'\n\nif dataset != 'simu':\n    X_train, X_val, X_test, y_train, y_val, y_test = get_data(dataset)\n    X_train = X_train[:, :1000]\n    X_test = X_test[:, :1000]\nelse:\n    X, y = make_regression(n_samples=100, n_features=100, noise=1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, test_size=0.5)\n\nn_samples, n_features = X_train.shape\n\nprint(\"Starting path computation...\")\nn_samples = len(y_train)\nalpha_max = np.max(np.abs(X_train.T.dot(y_train))) / X_train.shape[0]\nlog_alpha0 = np.log(alpha_max / 10)\n\nn_alphas = 10\np_alphas = np.geomspace(1, 0.0001, n_alphas)\nalphas = alpha_max * p_alphas\nlog_alphas = np.log(alphas)\n\ntol = 1e-7\nmax_iter = 1e5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid-search\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the solver of sklearn is indeed very long on the considered problems!\n# estimator = linear_model.Lasso(\n#     fit_intercept=False, max_iter=1000, warm_start=True)\n\n# celer is much more faster !\n# https://github.com/mathurinm/celer\n\nestimator = Lasso(\n    fit_intercept=False, max_iter=1000, warm_start=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grad-search\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('sparse-ho started')\n\nalpha0 = np.log(alpha_max / 10) * np.ones(n_features)\n\nt0 = time.time()\nmodel = wLasso(X_train, y_train, estimator=estimator)\n\n# here CV means held out\n# the \"real\" crossval (with folds etc) is very slow (for the moment) for some\n# unknown reasons\n\ncriterion = CV(X_val, y_val, model, X_test=X_test, y_test=y_test)\nalgo = ImplicitForward(criterion)\nmonitor_grad = Monitor()\ngrad_search(\n    algo, alpha0, monitor_grad, n_outer=10, tol=tol)\n\nt_grad_search = time.time() - t0\n\nprint(\"Time gradient serach:  %f\" % t_grad_search)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}