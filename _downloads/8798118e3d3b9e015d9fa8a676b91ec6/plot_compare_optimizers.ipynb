{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compare outer optimizers\n\nThis example shows how to perform hyperparameter optimization\nfor sparse logistic regression using a held-out test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>\n#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>\n#          Mathurin Massias\n#\n# License: BSD (3-clause)\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom celer import LogisticRegression\nfrom libsvmdata.datasets import fetch_libsvm\n\nfrom sparse_ho import ImplicitForward, Forward\nfrom sparse_ho.ho import grad_search\nfrom sparse_ho.utils import Monitor\nfrom sparse_ho.models import SparseLogreg\nfrom sparse_ho.criterion import HeldOutLogistic\nfrom sparse_ho.utils_plot import discrete_cmap\nfrom sparse_ho.grid_search import grid_search\nfrom sparse_ho.optimizers import LineSearch, GradientDescent, Adam\n\n\nprint(__doc__)\n\ndataset = 'rcv1_train'\n# dataset = 'simu'\n\nif dataset != 'simu':\n    X, y = fetch_libsvm(dataset)\n    X = X[:, :100]\nelse:\n    X, y = make_classification(\n        n_samples=100, n_features=1_000, random_state=42, flip_y=0.02)\n\n\nn_samples = X.shape[0]\nidx_train = np.arange(0, n_samples // 2)\nidx_val = np.arange(n_samples // 2, n_samples)\n\nn_samples = len(y[idx_train])\nalpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train]))\n\nalpha_max /= 2 * len(idx_train)\nalpha_max = alpha_max\nalpha_min = alpha_max / 100\nmax_iter = 100\n\ntol = 1e-8\n\nn_alphas = 30\np_alphas = np.geomspace(1, 0.0001, n_alphas)\nalphas = alpha_max * p_alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid-search\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = LogisticRegression(\n    penalty='l1', fit_intercept=False, max_iter=max_iter)\nmodel = SparseLogreg(max_iter=max_iter, estimator=estimator)\ncriterion = HeldOutLogistic(idx_train, idx_val)\nalgo_grid = Forward()\nmonitor_grid = Monitor()\ngrid_search(\n    algo_grid, criterion, model, X, y, alpha_min, alpha_max,\n    monitor_grid, alphas=alphas, tol=tol)\nobjs = np.array(monitor_grid.objs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grad-search\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer_names = ['line-search', 'gradient-descent', 'adam']\noptimizers = {\n    'line-search': LineSearch(n_outer=10, tol=tol),\n    'gradient-descent': GradientDescent(n_outer=10, step_size=100),\n    'adam': Adam(n_outer=10, lr=0.11)}\n\nmonitors = {}\nalpha0 = alpha_max / 10  # starting point\n\nfor optimizer_name in optimizer_names:\n    estimator = LogisticRegression(\n        penalty='l1', fit_intercept=False, solver='saga', tol=tol)\n    model = SparseLogreg(max_iter=max_iter, estimator=estimator)\n    criterion = HeldOutLogistic(idx_train, idx_val)\n\n    monitor_grad = Monitor()\n    algo = ImplicitForward(tol_jac=tol, n_iter_jac=1000)\n\n    optimizer = optimizers[optimizer_name]\n    grad_search(\n        algo, criterion, model, optimizer, X, y, alpha0,\n        monitor_grad)\n    monitors[optimizer_name] = monitor_grad\n\n\ncurrent_palette = sns.color_palette(\"colorblind\")\ndict_colors = {\n    'line-search': 'Greens',\n    'gradient-descent': 'Purples',\n    'adam': 'Reds'}\n\n\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(alphas / alphas[0], objs, color=current_palette[0])\nax.plot(\n    alphas / alphas[0], objs, 'bo',\n    label='0-order method (grid-search)', color=current_palette[1])\n\nfor optimizer_name in optimizer_names:\n    monitor = monitors[optimizer_name]\n    p_alphas_grad = np.array(monitor.alphas) / alpha_max\n    objs_grad = np.array(monitor.objs)\n    cmap = discrete_cmap(len(p_alphas_grad), dict_colors[optimizer_name])\n    ax.scatter(\n        p_alphas_grad, objs_grad, label=optimizer_name,\n        marker='X', color=cmap(np.linspace(0, 1, 10)), zorder=10)\n\nax.set_xlabel(r\"$\\lambda / \\lambda_{\\max}$\")\nax.set_ylabel(\n    r\"$ \\sum_i^n \\log \\left ( 1 + e^{-y_i^{\\rm{val}} X_i^{\\rm{val}} \"\n    r\"\\hat \\beta^{(\\lambda)} } \\right ) $\")\n\nax.set_xscale(\"log\")\nplt.tick_params(width=5)\nplt.legend()\nplt.tight_layout()\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}