{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lasso with Cross-validation\n\nThis example shows how to perform hyperparameter optimization\nfor a Lasso using a full cross-validation score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>\n#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>\n#\n# License: BSD (3-clause)\n\n# import time\n# import numpy as np\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import sklearn\n\n# from libsvmdata import fetch_libsvm\n# from sklearn.datasets import make_regression\n# from sklearn.linear_model import LassoCV\n# from sklearn.model_selection import KFold\n\n# from sparse_ho.models import Lasso\n# from sparse_ho.criterion import HeldOutMSE, CrossVal\n# from sparse_ho.implicit_forward import ImplicitForward\n# from sparse_ho.utils import Monitor\n# from sparse_ho.ho import grad_search\n\n# print(__doc__)\n\n# # dataset = 'rcv1'\n# dataset = 'simu'\n\n# if dataset == 'rcv1':\n#     X, y = fetch_libsvm('rcv1_train')\n# else:\n#     X, y = make_regression(\n#         n_samples=500, n_features=1000, noise=40,\n#         random_state=42)\n\n# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# print(\"Starting path computation...\")\n# n_samples = len(y)\n# alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples\n\n# n_alphas = 10\n# p_alphas = np.geomspace(1, 0.001, n_alphas)\n# alphas = alpha_max * p_alphas\n\n# tol = 1e-8\n\n# max_iter = 1e5\n\n# ##############################################################################\n# # Cross-validation with scikit-learn\n# # ----------------------------------\n# print('scikit started')\n\n# t0 = time.time()\n# reg = LassoCV(\n#     cv=kf, verbose=True, tol=tol, fit_intercept=False,\n#     alphas=alphas, max_iter=max_iter).fit(X, y)\n# reg.score(X, y)\n# t_sk = time.time() - t0\n\n# print('scikit finished')\n\n# ##############################################################################\n# # Now do the hyperparameter optimization with implicit differentiation\n# # --------------------------------------------------------------------\n\n# estimator = sklearn.linear_model.Lasso(\n#     fit_intercept=False, max_iter=1000, warm_start=True, tol=tol)\n\n# print('sparse-ho started')\n\n# t0 = time.time()\n# Model = Lasso\n# Criterion = HeldOutMSE\n# log_alpha0 = np.log(alpha_max / 10)\n# monitor_grad = Monitor()\n# criterion = CrossVal(X, y, Model, cv=kf, estimator=estimator)\n# algo = ImplicitForward()\n# grad_search(\n#     algo, criterion, np.log(alpha_max / 10), monitor_grad, n_outer=10, tol=tol)\n\n# t_grad_search = time.time() - t0\n\n# print('sparse-ho finished')\n\n# ##############################################################################\n# # Plot results\n# # ------------\n# objs = reg.mse_path_.mean(axis=1)\n\n# p_alphas_grad = np.exp(np.array(monitor_grad.log_alphas)) / alpha_max\n# objs_grad = np.array(monitor_grad.objs)\n\n\n# print(\"Time to compute CV for scikit-learn: %.2f\" % t_sk)\n# print(\"Time to compute CV for sparse-ho: %.2f\" % t_grad_search)\n\n# print('Minimum objective grid-search %.5f' % objs.min())\n# print('Minimum objective grad-search %.5f' % objs_grad.min())\n\n\n# current_palette = sns.color_palette(\"colorblind\")\n\n# fig = plt.figure(figsize=(5, 3))\n# plt.semilogx(\n#     p_alphas, objs, color=current_palette[0])\n# plt.semilogx(\n#     p_alphas, objs, 'bo', label='0-order method (grid-search)',\n#     color=current_palette[1])\n# plt.semilogx(\n#     p_alphas_grad, objs_grad, 'bX', label='1-st order method',\n#     color=current_palette[2])\n# plt.xlabel(r\"$\\lambda / \\lambda_{\\max}$\")\n# plt.ylabel(\"Cross-validation loss\")\n# axes = plt.gca()\n# plt.tick_params(width=5)\n# plt.legend()\n# plt.tight_layout()\n# plt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}