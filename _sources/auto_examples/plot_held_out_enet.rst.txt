
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_held_out_enet.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_held_out_enet.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_held_out_enet.py:


==================================
Elastic net with held-out test set
==================================

This example shows how to perform hyperparameter optimization
for an elastic-net using a held-out validation set.

.. GENERATED FROM PYTHON SOURCE LINES 10-36

.. code-block:: default


    # Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
    #          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
    #
    # License: BSD (3-clause)

    import time
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn import linear_model
    from libsvmdata.datasets import fetch_libsvm
    from celer.datasets import make_correlated_data
    from sklearn.metrics import mean_squared_error

    from sparse_ho import ImplicitForward
    from sparse_ho.criterion import HeldOutMSE
    from sparse_ho.models import ElasticNet
    from sparse_ho.ho import grad_search
    from sparse_ho.utils import Monitor
    from sparse_ho.utils_plot import discrete_cmap
    from sparse_ho.optimizers import GradientDescent


    # dataset = "rcv1"
    dataset = 'simu'








.. GENERATED FROM PYTHON SOURCE LINES 37-38

Load some data

.. GENERATED FROM PYTHON SOURCE LINES 38-71

.. code-block:: default


    # dataset = 'rcv1'
    dataset = 'simu'

    if dataset == 'rcv1':
        X, y = fetch_libsvm('rcv1_train')
        y -= y.mean()
        y /= np.linalg.norm(y)
    else:
        X, y, _ = make_correlated_data(
            n_samples=200, n_features=400, snr=5, random_state=0)


    n_samples = X.shape[0]
    idx_train = np.arange(0, n_samples // 2)
    idx_val = np.arange(n_samples // 2, n_samples)

    print("Starting path computation...")
    alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)

    alpha_min = 1e-4 * alpha_max

    n_grid = 10
    alphas_l1 = np.geomspace(alpha_max, alpha_min, n_grid)
    alphas_l2 = np.geomspace(alpha_max, alpha_min, n_grid)

    results = np.zeros((n_grid, n_grid))
    tol = 1e-5
    max_iter = 10_000

    estimator = linear_model.ElasticNet(
        fit_intercept=False, tol=tol, max_iter=max_iter, warm_start=True)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Starting path computation...




.. GENERATED FROM PYTHON SOURCE LINES 72-74

grid search with scikit-learn
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 74-89

.. code-block:: default


    print("Started grid search")
    t_grid_search = - time.time()
    for i in range(n_grid):
        print("lambda %i / %i" % (i * n_grid, n_grid * n_grid))
        for j in range(n_grid):
            estimator.alpha = (alphas_l1[i] + alphas_l2[j])
            estimator.l1_ratio = alphas_l1[i] / (alphas_l1[i] + alphas_l2[j])
            estimator.fit(X[idx_train, :], y[idx_train])
            results[i, j] = mean_squared_error(
                y[idx_val], estimator.predict(X[idx_val, :]))
    t_grid_search += time.time()
    print("Finished grid search")
    print("Minimum outer criterion value with grid search %0.3e" % results.min())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Started grid search
    lambda 0 / 100
    lambda 10 / 100
    lambda 20 / 100
    lambda 30 / 100
    lambda 40 / 100
    lambda 50 / 100
    lambda 60 / 100
    lambda 70 / 100
    lambda 80 / 100
    lambda 90 / 100
    Finished grid search
    Minimum outer criterion value with grid search 2.268e+01




.. GENERATED FROM PYTHON SOURCE LINES 90-92

Grad-search with sparse-ho
--------------------------

.. GENERATED FROM PYTHON SOURCE LINES 92-115

.. code-block:: default

    estimator = linear_model.ElasticNet(
        fit_intercept=False, max_iter=max_iter, warm_start=True)
    print("Started grad-search")
    t_grad_search = - time.time()
    monitor = Monitor()
    n_outer = 10
    alpha0 = np.array([alpha_max * 0.9, alpha_max * 0.9])
    model = ElasticNet(estimator=estimator)
    criterion = HeldOutMSE(idx_train, idx_val)
    algo = ImplicitForward(tol_jac=1e-3, n_iter_jac=100, max_iter=max_iter)
    optimizer = GradientDescent(
        n_outer=n_outer, tol=tol, p_grad0=1.5, verbose=True)
    grad_search(
        algo, criterion, model, optimizer, X, y, alpha0=alpha0,
        monitor=monitor)
    t_grad_search += time.time()
    monitor.alphas = np.array(monitor.alphas)

    print("Time grid search %f" % t_grid_search)
    print("Time grad-search %f" % t_grad_search)
    print("Minimum grid search %0.3e" % results.min())
    print("Minimum grad search %0.3e" % np.array(monitor.objs).min())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Started grad-search
    Iteration 1/10 ||Value outer criterion: 4.12e+01 ||norm grad 3.66e+00
    Iteration 2/10 ||Value outer criterion: 3.10e+01 ||norm grad 6.79e+00
    Iteration 3/10 ||Value outer criterion: 2.45e+01 ||norm grad 1.95e+00
    Iteration 4/10 ||Value outer criterion: 2.32e+01 ||norm grad 8.83e-01
    Iteration 5/10 ||Value outer criterion: 2.33e+01 ||norm grad 1.16e+00
    Iteration 6/10 ||Value outer criterion: 2.32e+01 ||norm grad 1.42e+00
    Iteration 7/10 ||Value outer criterion: 2.30e+01 ||norm grad 1.45e+00
    Iteration 8/10 ||Value outer criterion: 2.27e+01 ||norm grad 1.60e+00
    Iteration 9/10 ||Value outer criterion: 2.26e+01 ||norm grad 9.71e-02
    Iteration 10/10 ||Value outer criterion: 2.26e+01 ||norm grad 4.36e-02
    Time grid search 3.628204
    Time grad-search 2.109857
    Minimum grid search 2.268e+01
    Minimum grad search 2.257e+01




.. GENERATED FROM PYTHON SOURCE LINES 116-118

Plot results
------------

.. GENERATED FROM PYTHON SOURCE LINES 118-144

.. code-block:: default


    cmap = discrete_cmap(n_outer, 'Reds')
    X, Y = np.meshgrid(alphas_l1 / alpha_max, alphas_l2 / alpha_max)
    fig, ax = plt.subplots(1, 1)
    cp = ax.contourf(X, Y, results.T)
    ax.scatter(
        X, Y, s=10, c="orange", marker="o", label="$0$th order (grid search)",
        clip_on=False)
    ax.scatter(
        monitor.alphas[:, 0] / alpha_max, monitor.alphas[:, 1] / alpha_max,
        s=40, color=cmap(np.linspace(0, 1, n_outer)), zorder=10,
        marker="X", label="$1$st order")
    ax.plot(
        monitor.alphas[:, 0] / alpha_max, monitor.alphas[:, 1] / alpha_max,
        c=cmap(0))
    ax.set_xlim(X.min(), X.max())
    ax.set_xlabel("L1 regularization")
    ax.set_ylabel("L2 regularization")
    ax.set_ylim(Y.min(), Y.max())
    ax.set_title("Elastic net held out prediction loss on test set")
    cb = fig.colorbar(cp)
    cb.set_label("Held-out loss")
    plt.xscale('log')
    plt.yscale('log')
    plt.legend()
    plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_held_out_enet_001.png
    :alt: Elastic net held out prediction loss on test set
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  6.316 seconds)


.. _sphx_glr_download_auto_examples_plot_held_out_enet.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_held_out_enet.py <plot_held_out_enet.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_held_out_enet.ipynb <plot_held_out_enet.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
