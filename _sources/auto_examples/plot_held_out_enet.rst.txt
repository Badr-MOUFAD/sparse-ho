
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_held_out_enet.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_held_out_enet.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_held_out_enet.py:


==================================
Elastic net with held-out test set
==================================

This example shows how to perform hyperparameter optimization
for an elastic-net using a held-out validation set.

.. GENERATED FROM PYTHON SOURCE LINES 10-36

.. code-block:: default


    # Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
    #          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
    #
    # License: BSD (3-clause)

    import time
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn import linear_model
    from libsvmdata.datasets import fetch_libsvm
    from sklearn.datasets import make_regression
    from sklearn.metrics import mean_squared_error

    from sparse_ho import ImplicitForward
    from sparse_ho.criterion import HeldOutMSE
    from sparse_ho.models import ElasticNet
    from sparse_ho.ho import grad_search
    from sparse_ho.utils import Monitor
    from sparse_ho.utils_plot import discrete_cmap
    from sparse_ho.optimizers import GradientDescent


    # dataset = "rcv1"
    dataset = 'simu'








.. GENERATED FROM PYTHON SOURCE LINES 37-38

Load some data

.. GENERATED FROM PYTHON SOURCE LINES 38-71

.. code-block:: default


    # dataset = 'rcv1'
    dataset = 'simu'

    if dataset == 'rcv1':
        X, y = fetch_libsvm('rcv1_train')
        y -= y.mean()
        y /= np.linalg.norm(y)
    else:
        X, y = make_regression(
            n_samples=20, n_features=100, noise=1, random_state=42)


    n_samples = X.shape[0]
    idx_train = np.arange(0, n_samples // 2)
    idx_val = np.arange(n_samples // 2, n_samples)

    print("Starting path computation...")
    alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)

    alpha_min = 1e-4 * alpha_max

    n_grid = 10
    alphas_l1 = np.geomspace(alpha_max, alpha_min, n_grid)
    alphas_l2 = np.geomspace(alpha_max, alpha_min, n_grid)

    results = np.zeros((n_grid, n_grid))
    tol = 1e-5
    max_iter = 10_000

    estimator = linear_model.ElasticNet(
        fit_intercept=False, tol=tol, max_iter=max_iter, warm_start=True)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Starting path computation...




.. GENERATED FROM PYTHON SOURCE LINES 72-74

grid search with scikit-learn
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 74-89

.. code-block:: default


    print("Started grid search")
    t_grid_search = - time.time()
    for i in range(n_grid):
        print("lambda %i / %i" % (i * n_grid, n_grid * n_grid))
        for j in range(n_grid):
            estimator.alpha = (alphas_l1[i] + alphas_l2[j])
            estimator.l1_ratio = alphas_l1[i] / (alphas_l1[i] + alphas_l2[j])
            estimator.fit(X[idx_train, :], y[idx_train])
            results[i, j] = mean_squared_error(
                y[idx_val], estimator.predict(X[idx_val, :]))
    t_grid_search += time.time()
    print("Finished grid search")
    print("Minimum outer criterion value with grid search %0.3e" % results.min())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Started grid search
    lambda 0 / 100
    lambda 10 / 100
    lambda 20 / 100
    lambda 30 / 100
    lambda 40 / 100
    lambda 50 / 100
    lambda 60 / 100
    lambda 70 / 100
    lambda 80 / 100
    lambda 90 / 100
    Finished grid search
    Minimum outer criterion value with grid search 1.561e+04




.. GENERATED FROM PYTHON SOURCE LINES 90-92

Grad-search with sparse-ho
--------------------------

.. GENERATED FROM PYTHON SOURCE LINES 92-115

.. code-block:: default

    estimator = linear_model.ElasticNet(
        fit_intercept=False, max_iter=max_iter, warm_start=True)
    print("Started grad-search")
    t_grad_search = - time.time()
    monitor = Monitor()
    n_outer = 25
    alpha0 = np.array([alpha_max * 0.3, alpha_max / 10])
    model = ElasticNet(max_iter=max_iter, estimator=estimator)
    criterion = HeldOutMSE(idx_train, idx_val)
    algo = ImplicitForward(tol_jac=1e-3, n_iter_jac=100, max_iter=max_iter)
    optimizer = GradientDescent(
        n_outer=n_outer, tol=tol, p_grad0=1.5, verbose=True)
    grad_search(
        algo, criterion, model, optimizer, X, y, alpha0=alpha0,
        monitor=monitor)
    t_grad_search += time.time()
    monitor.alphas = np.array(monitor.alphas)

    print("Time grid search %f" % t_grid_search)
    print("Time grad-search %f" % t_grad_search)
    print("Minimum grid search %0.3e" % results.min())
    print("Minimum grad search %0.3e" % np.array(monitor.objs).min())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Started grad-search
    Iteration 1/25 ||Value outer criterion: 1.64e+04 ||norm grad 5.00e+02
    Iteration 2/25 ||Value outer criterion: 1.59e+04 ||norm grad 2.14e+02
    Iteration 3/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.73e+02
    Iteration 4/25 ||Value outer criterion: 1.57e+04 ||norm grad 7.03e+01
    Iteration 5/25 ||Value outer criterion: 1.58e+04 ||norm grad 4.84e+02
    Iteration 6/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.00e+01
    Iteration 7/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.00e+01
    Iteration 8/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.27e+01
    Iteration 9/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.30e+01
    Iteration 10/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.33e+01
    Iteration 11/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.38e+01
    Iteration 12/25 ||Value outer criterion: 1.57e+04 ||norm grad 6.43e+01
    Iteration 13/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.05e+02
    Iteration 14/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.09e+02
    Iteration 15/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.10e+02
    Iteration 16/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.15e+02
    Iteration 17/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.19e+02
    Iteration 18/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.24e+02
    Iteration 19/25 ||Value outer criterion: 1.57e+04 ||norm grad 1.30e+02
    Iteration 20/25 ||Value outer criterion: 1.56e+04 ||norm grad 1.97e+02
    Iteration 21/25 ||Value outer criterion: 1.56e+04 ||norm grad 2.01e+02
    Iteration 22/25 ||Value outer criterion: 1.56e+04 ||norm grad 2.33e+02
    Iteration 23/25 ||Value outer criterion: 1.56e+04 ||norm grad 3.13e+02
    Iteration 24/25 ||Value outer criterion: 1.56e+04 ||norm grad 1.03e+02
    Iteration 25/25 ||Value outer criterion: 1.56e+04 ||norm grad 1.33e+02
    Time grid search 0.091442
    Time grad-search 1.302695
    Minimum grid search 1.561e+04
    Minimum grad search 1.559e+04




.. GENERATED FROM PYTHON SOURCE LINES 116-118

Plot results
------------

.. GENERATED FROM PYTHON SOURCE LINES 118-141

.. code-block:: default


    cmap = discrete_cmap(n_outer, 'Greens')
    X, Y = np.meshgrid(alphas_l1 / alpha_max, alphas_l2 / alpha_max)
    fig, ax = plt.subplots(1, 1)
    cp = ax.contourf(X, Y, results.T)
    ax.scatter(
        X, Y, s=10, c="orange", marker="o", label="$0$th order (grid search)",
        clip_on=False)
    ax.scatter(
        monitor.alphas[:, 0] / alpha_max, monitor.alphas[:, 1] / alpha_max,
        s=40, color=cmap(np.linspace(0, 1, n_outer)), zorder=10,
        marker="X", label="$1$st order")
    ax.set_xlim(X.min(), X.max())
    ax.set_xlabel("L1 regularization")
    ax.set_ylabel("L2 regularization")
    ax.set_ylim(Y.min(), Y.max())
    ax.set_title("Elastic net held out prediction loss on test set")
    cb = fig.colorbar(cp)
    cb.set_label("Held-out loss")
    plt.xscale('log')
    plt.yscale('log')
    plt.legend()
    plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_held_out_enet_001.png
    :alt: Elastic net held out prediction loss on test set
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.851 seconds)


.. _sphx_glr_download_auto_examples_plot_held_out_enet.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_held_out_enet.py <plot_held_out_enet.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_held_out_enet.ipynb <plot_held_out_enet.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
