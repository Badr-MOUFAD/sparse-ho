
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_wlasso.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_wlasso.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_wlasso.py:


=====================================
Weighted Lasso with held-out test set
=====================================

This example shows how to perform hyperparameter optimization
for a weighted Lasso using a held-out validation set.
In particular we compare the weighted Lasso to LassoCV on a toy example

.. GENERATED FROM PYTHON SOURCE LINES 10-35

.. code-block:: default


    # Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
    #          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
    #          Kenan Sehic
    #          Mathurin Massias
    # License: BSD (3-clause)

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error
    from sklearn.model_selection import KFold
    from celer import Lasso, LassoCV
    from celer.datasets import make_correlated_data

    from sparse_ho.models import WeightedLasso
    from sparse_ho.criterion import HeldOutMSE, CrossVal
    from sparse_ho import ImplicitForward
    from sparse_ho.utils import Monitor
    from sparse_ho.ho import grad_search
    from sparse_ho.optimizers import GradientDescent









.. GENERATED FROM PYTHON SOURCE LINES 36-37

Dataset creation

.. GENERATED FROM PYTHON SOURCE LINES 37-40

.. code-block:: default

    X, y, w_true = make_correlated_data(
        n_samples=100, n_features=1000, random_state=0, snr=5)








.. GENERATED FROM PYTHON SOURCE LINES 41-46

.. code-block:: default

    X, X_test, y, y_test = train_test_split(X, y, test_size=0.333, random_state=0)

    n_samples = X.shape[0]
    idx_train = np.arange(0, n_samples // 2)
    idx_val = np.arange(n_samples // 2, n_samples)







.. GENERATED FROM PYTHON SOURCE LINES 49-50

Max penalty value

.. GENERATED FROM PYTHON SOURCE LINES 50-53

.. code-block:: default

    alpha_max = np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)
    n_alphas = 30
    alphas = np.geomspace(alpha_max, alpha_max / 1_000, n_alphas)







.. GENERATED FROM PYTHON SOURCE LINES 54-56

.. code-block:: default


    # Create cross validation object







.. GENERATED FROM PYTHON SOURCE LINES 57-58

.. code-block:: default

    cv = KFold(n_splits=5, shuffle=True, random_state=42)







.. GENERATED FROM PYTHON SOURCE LINES 61-62

Vanilla LassoCV

.. GENERATED FROM PYTHON SOURCE LINES 62-70

.. code-block:: default

    print("========== Celer's LassoCV started ===============")
    model_cv = LassoCV(
        verbose=False, fit_intercept=False, alphas=alphas, tol=1e-7, max_iter=100,
        cv=cv, n_jobs=2).fit(X, y)

    # Measure mse on test
    mse_cv = mean_squared_error(y_test, model_cv.predict(X_test))
    print("Vanilla LassoCV: Mean-squared error on test data %f" % mse_cv)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ========== Celer's LassoCV started ===============
    Vanilla LassoCV: Mean-squared error on test data 164.404580




.. GENERATED FROM PYTHON SOURCE LINES 74-76

Weighted Lasso with sparse-ho.
We use the vanilla lassoCV coefficients as a starting point

.. GENERATED FROM PYTHON SOURCE LINES 76-87

.. code-block:: default

    log_alpha0 = np.log(model_cv.alpha_) * np.ones(X.shape[1])
    # Weighted Lasso: Sparse-ho: 1 param per feature
    estimator = Lasso(fit_intercept=False, max_iter=100, warm_start=True)
    model = WeightedLasso(estimator=estimator)
    sub_criterion = HeldOutMSE(idx_train, idx_val)
    criterion = CrossVal(sub_criterion, cv=cv)
    algo = ImplicitForward()
    monitor = Monitor()
    optimizer = GradientDescent(n_outer=50, tol=1e-7, verbose=True, p_grad0=1.9)
    log_alphaopt, _, _ = grad_search(
        algo, criterion, model, optimizer, X, y, log_alpha0, monitor)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Iteration 1/50 ||Value outer criterion: 2.24e+02 ||norm grad 1.23e+02
    Iteration 2/50 ||Value outer criterion: 1.23e+02 ||norm grad 4.28e+01
    Iteration 3/50 ||Value outer criterion: 9.70e+01 ||norm grad 2.73e+01
    Iteration 4/50 ||Value outer criterion: 6.87e+01 ||norm grad 1.10e+01
    Iteration 5/50 ||Value outer criterion: 5.88e+01 ||norm grad 4.58e+00
    Iteration 6/50 ||Value outer criterion: 5.28e+01 ||norm grad 4.10e+00
    Iteration 7/50 ||Value outer criterion: 5.09e+01 ||norm grad 2.01e+00
    Iteration 8/50 ||Value outer criterion: 4.99e+01 ||norm grad 7.85e-01
    Iteration 9/50 ||Value outer criterion: 4.93e+01 ||norm grad 3.86e-01
    Iteration 10/50 ||Value outer criterion: 4.94e+01 ||norm grad 9.43e-01
    Iteration 11/50 ||Value outer criterion: 4.93e+01 ||norm grad 4.25e-01
    Iteration 12/50 ||Value outer criterion: 4.92e+01 ||norm grad 3.88e-01
    Iteration 13/50 ||Value outer criterion: 4.92e+01 ||norm grad 3.65e-01
    Iteration 14/50 ||Value outer criterion: 4.92e+01 ||norm grad 3.47e-01
    Iteration 15/50 ||Value outer criterion: 4.92e+01 ||norm grad 3.30e-01
    Iteration 16/50 ||Value outer criterion: 4.91e+01 ||norm grad 3.14e-01
    Iteration 17/50 ||Value outer criterion: 4.91e+01 ||norm grad 2.98e-01
    Iteration 18/50 ||Value outer criterion: 4.91e+01 ||norm grad 2.81e-01
    Iteration 19/50 ||Value outer criterion: 4.91e+01 ||norm grad 2.64e-01
    Iteration 20/50 ||Value outer criterion: 4.91e+01 ||norm grad 2.47e-01
    Iteration 21/50 ||Value outer criterion: 4.91e+01 ||norm grad 2.30e-01
    Iteration 22/50 ||Value outer criterion: 4.90e+01 ||norm grad 2.13e-01
    Iteration 23/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.98e-01
    Iteration 24/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.84e-01
    Iteration 25/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.72e-01
    Iteration 26/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.62e-01
    Iteration 27/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.54e-01
    Iteration 28/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.48e-01
    Iteration 29/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.43e-01
    Iteration 30/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.38e-01
    Iteration 31/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.35e-01
    Iteration 32/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.32e-01
    Iteration 33/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.29e-01
    Iteration 34/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.26e-01
    Iteration 35/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.24e-01
    Iteration 36/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.22e-01
    Iteration 37/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.19e-01
    Iteration 38/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.17e-01
    Iteration 39/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.15e-01
    Iteration 40/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.13e-01
    Iteration 41/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.10e-01
    Iteration 42/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.08e-01
    Iteration 43/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.06e-01
    Iteration 44/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.03e-01
    Iteration 45/50 ||Value outer criterion: 4.90e+01 ||norm grad 1.01e-01
    Iteration 46/50 ||Value outer criterion: 4.90e+01 ||norm grad 9.90e-02
    Iteration 47/50 ||Value outer criterion: 4.90e+01 ||norm grad 9.68e-02
    Iteration 48/50 ||Value outer criterion: 4.89e+01 ||norm grad 9.46e-02
    Iteration 49/50 ||Value outer criterion: 4.89e+01 ||norm grad 9.25e-02
    Iteration 50/50 ||Value outer criterion: 4.89e+01 ||norm grad 9.05e-02




.. GENERATED FROM PYTHON SOURCE LINES 88-91

.. code-block:: default


    estimator.weights = np.exp(log_alphaopt)
    estimator.fit(X, y)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Lasso(fit_intercept=False, warm_start=True,
          weights=array([2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.48987248, 0.23907301, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724...
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498,
           2.78724498, 2.78724498, 2.78724498, 2.78724498, 2.78724498]))



.. GENERATED FROM PYTHON SOURCE LINES 92-93

MSE on validation set

.. GENERATED FROM PYTHON SOURCE LINES 93-115

.. code-block:: default

    mse_sho_val = mean_squared_error(y, estimator.predict(X))

    # MSE on test set, ie unseen data
    mse_sho_test = mean_squared_error(y_test, estimator.predict(X_test))

    # Oracle MSE
    mse_oracle = mean_squared_error(y_test, X_test @ w_true)

    print("Sparse-ho: Mean-squared error on validation data %f" % mse_sho_val)
    print("Sparse-ho: Mean-squared error on test (unseen) data %f" % mse_sho_test)


    labels = ['WeightedLasso val', 'WeightedLasso test', 'Lasso CV', 'Oracle']

    df = pd.DataFrame(
        np.array([mse_sho_val, mse_sho_test, mse_cv, mse_oracle]).reshape((1, -1)),
        columns=labels)
    df.plot.bar(rot=0)
    plt.xlabel("Estimator")
    plt.ylabel("Mean squared error")
    plt.tight_layout()
    plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_wlasso_001.png
    :alt: plot wlasso
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Sparse-ho: Mean-squared error on validation data 36.508267
    Sparse-ho: Mean-squared error on test (unseen) data 175.489630





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  3.750 seconds)


.. _sphx_glr_download_auto_examples_plot_wlasso.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_wlasso.py <plot_wlasso.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_wlasso.ipynb <plot_wlasso.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
