
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_held_out_lasso.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_held_out_lasso.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_held_out_lasso.py:


============================
Lasso with held-out test set
============================

This example shows how to perform hyperparameter optimization
for a Lasso using a held-out validation set.

.. GENERATED FROM PYTHON SOURCE LINES 10-66

.. code-block:: default


    # Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
    #          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
    #
    # License: BSD (3-clause)

    import time

    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns

    from sklearn import linear_model
    from sklearn.datasets import make_regression


    from sparse_ho.models import Lasso
    from sparse_ho.criterion import HeldOutMSE
    from sparse_ho import Forward
    from sparse_ho import ImplicitForward
    from sparse_ho.utils import Monitor
    from sparse_ho.ho import grad_search
    from sparse_ho.grid_search import grid_search
    from sparse_ho.optimizers import LineSearch

    from libsvmdata.datasets import fetch_libsvm


    print(__doc__)

    dataset = 'rcv1'
    # dataset = 'simu'

    if dataset == 'rcv1':
        X, y = fetch_libsvm('rcv1_train')
    else:
        X, y = make_regression(n_samples=1000, n_features=1000, noise=40)

    n_samples = X.shape[0]
    idx_train = np.arange(0, n_samples // 2)
    idx_val = np.arange(n_samples // 2, n_samples)

    print("Starting path computation...")
    n_samples = len(y[idx_train])
    alpha_max = np.max(np.abs(X[idx_train, :].T.dot(y[idx_train])))
    alpha_max /= len(idx_train)
    log_alpha0 = np.log(alpha_max / 10)

    n_alphas = 10
    p_alphas = np.geomspace(1, 0.0001, n_alphas)
    alphas = alpha_max * p_alphas
    log_alphas = np.log(alphas)

    tol = 1e-7
    max_iter = 1e3





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    could import smt.sampling_methods

    Dataset: rcv1_train
    Downloading data from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2 (13.1 MB)

    file_sizes:   0%|                                   | 0.00/13.7M [00:00<?, ?B/s]    file_sizes:   0%|                           | 24.6k/13.7M [00:00<02:09, 106kB/s]    file_sizes:   0%|                           | 49.2k/13.7M [00:00<02:09, 106kB/s]    file_sizes:   1%|2                           | 106k/13.7M [00:00<01:19, 171kB/s]    file_sizes:   1%|3                           | 188k/13.7M [00:00<00:55, 243kB/s]    file_sizes:   4%|#                           | 500k/13.7M [00:01<00:20, 638kB/s]    file_sizes:   5%|#4                          | 696k/13.7M [00:01<00:19, 661kB/s]    file_sizes:  12%|###1                      | 1.68M/13.7M [00:01<00:08, 1.43MB/s]    file_sizes:  18%|####6                     | 2.47M/13.7M [00:02<00:05, 1.94MB/s]    file_sizes:  20%|#####1                    | 2.73M/13.7M [00:02<00:06, 1.72MB/s]    file_sizes:  25%|######4                   | 3.38M/13.7M [00:02<00:05, 2.03MB/s]    file_sizes:  28%|#######1                  | 3.78M/13.7M [00:02<00:05, 1.94MB/s]    file_sizes:  29%|#######5                  | 3.97M/13.7M [00:03<00:06, 1.61MB/s]    file_sizes:  32%|########2                 | 4.37M/13.7M [00:03<00:05, 1.64MB/s]    file_sizes:  38%|##########                | 5.28M/13.7M [00:03<00:03, 2.32MB/s]    file_sizes:  40%|##########5               | 5.55M/13.7M [00:03<00:04, 1.97MB/s]    file_sizes:  43%|###########2              | 5.94M/13.7M [00:03<00:04, 1.88MB/s]    file_sizes:  45%|###########6              | 6.14M/13.7M [00:04<00:04, 1.58MB/s]    file_sizes:  48%|############3             | 6.53M/13.7M [00:04<00:04, 1.61MB/s]    file_sizes:  49%|############7             | 6.73M/13.7M [00:04<00:06, 1.07MB/s]    file_sizes:  56%|##############4           | 7.64M/13.7M [00:05<00:05, 1.19MB/s]    file_sizes:  58%|##############9           | 7.91M/13.7M [00:05<00:04, 1.18MB/s]    file_sizes:  60%|###############7          | 8.30M/13.7M [00:06<00:04, 1.24MB/s]    file_sizes:  61%|###############9          | 8.43M/13.7M [00:06<00:04, 1.13MB/s]    file_sizes:  63%|################3         | 8.63M/13.7M [00:06<00:04, 1.06MB/s]    file_sizes:  65%|################9         | 8.95M/13.7M [00:06<00:04, 1.15MB/s]    file_sizes:  67%|#################3        | 9.15M/13.7M [00:06<00:04, 1.07MB/s]    file_sizes:  69%|##################6        | 9.48M/13.7M [00:07<00:04, 905kB/s]    file_sizes:  72%|##################8       | 9.94M/13.7M [00:07<00:03, 1.16MB/s]    file_sizes:  73%|###################       | 10.1M/13.7M [00:07<00:03, 1.01MB/s]    file_sizes:  75%|####################1      | 10.3M/13.7M [00:08<00:03, 967kB/s]    file_sizes:  76%|####################5      | 10.5M/13.7M [00:08<00:03, 891kB/s]    file_sizes:  78%|####################9      | 10.7M/13.7M [00:08<00:03, 920kB/s]    file_sizes:  79%|#####################2     | 10.8M/13.7M [00:08<00:03, 818kB/s]    file_sizes:  79%|#####################4     | 10.9M/13.7M [00:09<00:04, 706kB/s]    file_sizes:  81%|#####################8     | 11.1M/13.7M [00:09<00:03, 786kB/s]    file_sizes:  82%|######################1    | 11.2M/13.7M [00:09<00:03, 721kB/s]    file_sizes:  83%|######################4    | 11.4M/13.7M [00:09<00:03, 716kB/s]    file_sizes:  84%|######################7    | 11.6M/13.7M [00:09<00:03, 713kB/s]    file_sizes:  85%|#######################    | 11.7M/13.7M [00:10<00:02, 712kB/s]    file_sizes:  87%|#######################4   | 11.9M/13.7M [00:10<00:02, 710kB/s]    file_sizes:  88%|#######################7   | 12.1M/13.7M [00:10<00:02, 709kB/s]    file_sizes:  89%|########################   | 12.2M/13.7M [00:10<00:02, 709kB/s]    file_sizes:  90%|########################3  | 12.4M/13.7M [00:11<00:01, 709kB/s]    file_sizes:  91%|########################6  | 12.6M/13.7M [00:11<00:01, 707kB/s]    file_sizes:  93%|#########################  | 12.7M/13.7M [00:11<00:01, 701kB/s]    file_sizes:  94%|#########################4 | 13.0M/13.7M [00:11<00:00, 786kB/s]    file_sizes:  95%|#########################7 | 13.1M/13.7M [00:12<00:00, 717kB/s]    file_sizes:  96%|########################## | 13.2M/13.7M [00:12<00:00, 714kB/s]    file_sizes:  98%|##########################3| 13.4M/13.7M [00:12<00:00, 708kB/s]    file_sizes:  99%|##########################6| 13.6M/13.7M [00:12<00:00, 709kB/s]    file_sizes: 100%|###########################| 13.7M/13.7M [00:13<00:00, 704kB/s]    file_sizes: 100%|##########################| 13.7M/13.7M [00:13<00:00, 1.06MB/s]
    Successfully downloaded file to /home/circleci/data/libsvm/binary/rcv1_train.binary.bz2
    Decompressing...
    Loading svmlight file...
    Starting path computation...




.. GENERATED FROM PYTHON SOURCE LINES 67-69

Grid-search with scikit-learn
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 69-89

.. code-block:: default


    estimator = linear_model.Lasso(
        fit_intercept=False, max_iter=max_iter, warm_start=True)

    print('scikit-learn started')

    t0 = time.time()
    model = Lasso(estimator=estimator)
    criterion = HeldOutMSE(idx_train, idx_val)
    algo = Forward()
    monitor_grid_sk = Monitor()
    grid_search(
        algo, criterion, model, X, y, None, None, monitor_grid_sk,
        log_alphas=log_alphas, tol=tol)
    objs = np.array(monitor_grid_sk.objs)
    t_sk = time.time() - t0

    print('scikit-learn finished')






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    scikit-learn started
    Iteration 1 / 10
    Iteration 2 / 10
    Iteration 3 / 10
    Iteration 4 / 10
    Iteration 5 / 10
    Iteration 6 / 10
    Iteration 7 / 10
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06099701478785846, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    Iteration 8 / 10
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0066418148676917, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    Iteration 9 / 10
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.17218713503172, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    Iteration 10 / 10
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.79327661352437, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    scikit-learn finished




.. GENERATED FROM PYTHON SOURCE LINES 90-92

Grad-search with sparse-ho
--------------------------

.. GENERATED FROM PYTHON SOURCE LINES 92-108

.. code-block:: default


    print('sparse-ho started')

    t0 = time.time()
    model = Lasso(estimator=estimator)
    criterion = HeldOutMSE(idx_train, idx_val)
    algo = ImplicitForward(criterion)
    monitor_grad = Monitor()
    optimizer = LineSearch(n_outer=10, tol=tol)
    grad_search(
        algo, criterion, model, optimizer, X, y, log_alpha0, monitor_grad)

    t_grad_search = time.time() - t0

    print('sparse-ho finished')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    sparse-ho started
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0023956803358942125, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0030242011171139893, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    /home/circleci/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.003221665278488217, tolerance: 0.0010121
      model = cd_fast.sparse_enet_coordinate_descent(
    sparse-ho finished




.. GENERATED FROM PYTHON SOURCE LINES 109-111

Plot results
------------

.. GENERATED FROM PYTHON SOURCE LINES 111-142

.. code-block:: default


    p_alphas_grad = np.exp(np.array(monitor_grad.log_alphas)) / alpha_max

    objs_grad = np.array(monitor_grad.objs)

    print('sparse-ho finished')
    print("Time to compute CV for scikit-learn: %.2f" % t_sk)
    print("Time to compute CV for sparse-ho: %.2f" % t_grad_search)

    print('Minimum objective grid-search %.5f' % objs.min())
    print('Minimum objective grad-search %.5f' % objs_grad.min())


    current_palette = sns.color_palette("colorblind")

    plt.figure(figsize=(5, 3))
    plt.semilogx(
        p_alphas, objs, color=current_palette[0])
    plt.semilogx(
        p_alphas, objs, 'bo', label='0-order method (grid-search)',
        color=current_palette[1])
    plt.semilogx(
        p_alphas_grad, objs_grad, 'bX', label='1-st order method',
        color=current_palette[2])
    plt.xlabel(r"$\lambda / \lambda_{\max}$")
    plt.ylabel(
        r"$\|y^{\rm{val}} - X^{\rm{val}} \hat \beta^{(\lambda)} \|^2$")
    plt.tick_params(width=5)
    plt.legend()
    plt.tight_layout()
    plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_held_out_lasso_001.png
    :alt: plot held out lasso
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    sparse-ho finished
    Time to compute CV for scikit-learn: 13.29
    Time to compute CV for sparse-ho: 23.47
    Minimum objective grid-search 0.21316
    Minimum objective grad-search 0.21211





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  59.195 seconds)


.. _sphx_glr_download_auto_examples_plot_held_out_lasso.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_held_out_lasso.py <plot_held_out_lasso.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_held_out_lasso.ipynb <plot_held_out_lasso.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
