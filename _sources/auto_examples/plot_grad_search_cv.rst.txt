.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_grad_search_cv.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_plot_grad_search_cv.py:


=============================
Lasso with Cross-validation
=============================

This example shows how to perform hyperparameter optimization
for a Lasso using a full cross-validation score.


.. code-block:: default


    # Authors: Quentin Bertrand <quentin.bertrand@inria.fr>
    #          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>
    #
    # License: BSD (3-clause)

    import time
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import sklearn

    from libsvmdata import fetch_libsvm
    from sklearn.datasets import make_regression
    from sklearn.linear_model import LassoCV
    from sklearn.model_selection import KFold

    from sparse_ho.models import Lasso
    from sparse_ho.criterion import HeldOutMSE, CrossVal
    from sparse_ho.implicit_forward import ImplicitForward
    from sparse_ho.utils import Monitor
    from sparse_ho.ho import grad_search

    print(__doc__)

    # dataset = 'rcv1'
    dataset = 'simu'

    if dataset == 'rcv1':
        X, y = fetch_libsvm('rcv1_train')
    else:
        X, y = make_regression(
            n_samples=500, n_features=1000, noise=40,
            random_state=42)

    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    print("Starting path computation...")
    n_samples = len(y)
    alpha_max = np.max(np.abs(X.T.dot(y))) / n_samples

    n_alphas = 10
    p_alphas = np.geomspace(1, 0.001, n_alphas)
    alphas = alpha_max * p_alphas

    tol = 1e-8

    max_iter = 1e5





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Starting path computation...




Cross-validation with scikit-learn
----------------------------------


.. code-block:: default

    print('scikit started')

    t0 = time.time()
    reg = LassoCV(
        cv=kf, verbose=True, tol=tol, fit_intercept=False,
        alphas=alphas, max_iter=max_iter).fit(X, y)
    reg.score(X, y)
    t_sk = time.time() - t0

    print('scikit finished')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    scikit started
    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    ..................................................[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.5s finished
    scikit finished




Now do the hyperparameter optimization with implicit differentiation
--------------------------------------------------------------------


.. code-block:: default


    estimator = sklearn.linear_model.Lasso(
        fit_intercept=False, max_iter=1000, warm_start=True, tol=tol)

    print('sparse-ho started')

    t0 = time.time()
    Model = Lasso
    Criterion = HeldOutMSE
    log_alpha0 = np.log(alpha_max / 10)
    monitor_grad = Monitor()
    criterion = CrossVal(X, y, Model, cv=kf, estimator=estimator)
    algo = ImplicitForward()
    grad_search(
        algo, criterion, np.log(alpha_max / 10), monitor_grad, n_outer=10, tol=tol)

    t_grad_search = time.time() - t0

    print('sparse-ho finished')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    sparse-ho started
    sparse-ho finished




Plot results
------------


.. code-block:: default

    objs = reg.mse_path_.mean(axis=1)

    p_alphas_grad = np.exp(np.array(monitor_grad.log_alphas)) / alpha_max
    objs_grad = np.array(monitor_grad.objs)


    print("Time to compute CV for scikit-learn: %.2f" % t_sk)
    print("Time to compute CV for sparse-ho: %.2f" % t_grad_search)

    print('Minimum objective grid-search %.5f' % objs.min())
    print('Minimum objective grad-search %.5f' % objs_grad.min())


    current_palette = sns.color_palette("colorblind")

    fig = plt.figure(figsize=(5, 3))
    plt.semilogx(
        p_alphas, objs, color=current_palette[0])
    plt.semilogx(
        p_alphas, objs, 'bo', label='0-order method (grid-search)',
        color=current_palette[1])
    plt.semilogx(
        p_alphas_grad, objs_grad, 'bX', label='1-st order method',
        color=current_palette[2])
    plt.xlabel(r"$\lambda / \lambda_{\max}$")
    plt.ylabel("Cross-validation loss")
    axes = plt.gca()
    plt.tick_params(width=5)
    plt.legend()
    plt.tight_layout()
    plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_grad_search_cv_001.png
    :alt: plot grad search cv
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Time to compute CV for scikit-learn: 8.54
    Time to compute CV for sparse-ho: 1.70
    Minimum objective grid-search 1938.13556
    Minimum objective grad-search 1932.51526





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  10.871 seconds)


.. _sphx_glr_download_auto_examples_plot_grad_search_cv.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_grad_search_cv.py <plot_grad_search_cv.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_grad_search_cv.ipynb <plot_grad_search_cv.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
